# LLM Model Details

Technical specifications for major language models.

## Anthropic

### Claude Sonnet 4.5
- Context: 200K tokens
- Max Output: 8,192 tokens
- Capabilities: Function calling, vision, JSON mode, extended thinking
- API: claude-sonnet-4-5-20250929

### Claude Opus 4.1
- Context: 200K tokens
- Max Output: 4,096 tokens
- Capabilities: Function calling, vision, JSON mode
- API: claude-opus-4-1-20250514

### Claude Haiku 4
- Context: 200K tokens
- Max Output: 4,096 tokens
- Capabilities: Function calling, vision, JSON mode
- API: claude-haiku-4-20250514

## OpenAI

### GPT-4o
- Context: 128K tokens
- Max Output: 4,096 tokens
- Capabilities: Function calling, vision, JSON mode, voice
- API: gpt-4o-2024-08-06

### GPT-4o-mini
- Context: 128K tokens
- Max Output: 4,096 tokens
- Capabilities: Function calling, vision, JSON mode
- API: gpt-4o-mini-2024-07-18

### o1-preview
- Context: 128K tokens
- Max Output: 32K tokens
- Capabilities: Extended reasoning
- API: o1-2024-12-17

## Google

### Gemini 1.5 Pro
- Context: 2M tokens
- Max Output: 8,192 tokens
- Capabilities: Function calling, vision, audio, video
- API: gemini-1.5-pro-002

### Gemini 1.5 Flash
- Context: 1M tokens
- Max Output: 8,192 tokens
- Capabilities: Function calling, vision, audio, video
- API: gemini-1.5-flash-002

## Meta

### Llama 3.1 405B
- Context: 128K tokens
- Open source
- Via third-party providers (Together, Replicate)

## Mistral

### Mistral Large 2
- Context: 128K tokens
- Max Output: 8,192 tokens
- API: mistral-large-2407

## DeepSeek

### DeepSeek Coder V2
- Context: 128K tokens
- Specialized for code generation
- API: deepseek-coder
