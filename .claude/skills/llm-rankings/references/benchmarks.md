# LLM Benchmark Scores

Comprehensive benchmark scores across major language models.

## Reasoning & General Intelligence

### MMLU (Massive Multitask Language Understanding)
| Model | Score | Provider |
|-------|-------|----------|
| Claude Opus 4.1 | 88.7% | Anthropic |
| Claude Sonnet 4.5 | 88.3% | Anthropic |
| GPT-4o | 88.7% | OpenAI |
| Gemini 1.5 Pro | 85.9% | Google |
| Llama 3.1 405B | 88.6% | Meta |

### GSM8K (Grade School Math)
| Model | Score | Provider |
|-------|-------|----------|
| Claude Sonnet 4.5 | 96.4% | Anthropic |
| GPT-4o | 96.1% | OpenAI |
| Claude Opus 4.1 | 95.0% | Anthropic |

## Code Generation

### HumanEval
| Model | Score | Provider |
|-------|-------|----------|
| Claude Sonnet 4.5 | 92.0% | Anthropic |
| GPT-4o | 90.2% | OpenAI |
| DeepSeek Coder V2 | 90.2% | DeepSeek |
| Llama 3.1 405B | 89.0% | Meta |

## Multimodal Performance

### MMMU
| Model | Score | Provider |
|-------|-------|----------|
| Claude Sonnet 4.5 | 70.4% | Anthropic |
| GPT-4o | 69.1% | OpenAI |
| Gemini 1.5 Pro | 62.2% | Google |

## Context Windows
| Model | Tokens | Provider |
|-------|--------|----------|
| Gemini 1.5 Pro | 2M | Google |
| Gemini 1.5 Flash | 1M | Google |
| Claude models | 200K | Anthropic |
| GPT-4 models | 128K | OpenAI |
